---
title: "Parkinson Classification Based on Demographic Information and Voice Features"
author: "Bowen Xiao"
date: "May 22, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Using demographic information and GeMAPS extracted features of voice to classify a patient diagnosed with Parkinsonâ€™s disease. I started with demogrphics data (~80% negative cases), testing different classification methods (achieved ~90% accuracy), and then went on combining with voice features (~60% negative cases). Finally, I used a hierarchical regularized logistic regression and achieved 90% accuracy and 86% recall.

Also, I packed the combined data to a simple neural network (single hidden layer with some dropout, mini batch and scale adjustment), I set the estimated parameter of logistic regression with demographic data as initial weights and it turned out to be a tiny improvement (achieved ~90% accuracy and ~80% callback).

# Data Preparation

Demographic dataset has 6627 records and about 84% are negative cases. I split the data into train set (80%) and test set (20%). A quick look of the data is also shown as following.

```{r results='asis'}
setwd("~/Parkinson_Classification_Based_on_Demographic_Information_and_Voice_Features")
parkinson <- read.csv("./src/R/parkinson.csv")
parkinson$brain<-as.factor(parkinson$brain)
parkinson$edu<-as.factor(parkinson$edu)
parkinson$emp<-as.factor(parkinson$emp)
parkinson$gender<-as.factor(parkinson$gender)
parkinson$mar<-as.factor(parkinson$mar)
parkinson$race<-as.factor(parkinson$race)
parkinson$smoke<-as.factor(parkinson$smoke)
parkinson$diag<-as.factor(parkinson$diag)

#visulazation
round(nrow(parkinson[parkinson$diag=='FALSE',])/nrow(parkinson),2)
library(knitr)
kable(round(prop.table(table(parkinson$brain,parkinson$diag)),2))
kable(round(prop.table(table(parkinson$edu,parkinson$diag)),2))
kable(round(prop.table(table(parkinson$emp,parkinson$diag)),2))
kable(round(prop.table(table(parkinson$gender,parkinson$diag)),2))
kable(round(prop.table(table(parkinson$mar,parkinson$diag)),2))
kable(round(prop.table(table(parkinson$race,parkinson$diag)),2))
kable(round(prop.table(table(parkinson$smoke,parkinson$diag)),2))

# split it into train set and test set
set.seed(123)
index=sample(1:nrow(parkinson),0.8*nrow(parkinson))
parkinson_train<-parkinson[index,]
parkinson_test<-parkinson[-index,]
```

\newpage

# Analysis of Demographic Information

## Logistic Regression



```{r}
model1<-glm(diag~.,data=parkinson_train,family=binomial(link='logit'))
y_1<-predict.glm(model1,newdata = parkinson_test,type='response')
y_1=ifelse(y_1>=0.5,'TRUE','FALSE')
A1<-mean(y_1==parkinson_test$diag)
R1<-mean(y_1[parkinson_test$diag=='TRUE']==parkinson_test[parkinson_test$diag=='TRUE',]$diag)
```

```{r fig.height=8}
library(effects)
plot(allEffects(model1))
```

As is shown, `gender` and `race` seem not to be significant predictor.


## SVM

```{r}
library(e1071)
svmfit<-svm(diag~.,data=parkinson_train,kernel="radial")
svmpred<-predict(svmfit,newdata = parkinson_test)
A2<-mean(svmpred==parkinson_test$diag)
R2<-mean(svmpred[parkinson_test$diag=='TRUE']==parkinson_test[parkinson_test$diag=='TRUE',]$diag)
```

A simple model with age can achieve 89% accuracy.

```{r warning=FALSE}
svmage<-svm(diag~age,data=parkinson_train,kernel="radial",probability=TRUE)
ppred<-predict(svmage,newdata = parkinson_test)
round(mean(ppred==parkinson_test$diag),2)
pred <- predict(svmage, parkinson_train, decision.values = TRUE, probability = TRUE)
plot(parkinson_train$age[order(parkinson_train$age)],
     attr(pred, "probabilities")[,2][order(parkinson_train$age)],
     xlab='age',type='l',ylab='probability',col=2,lwd=2)
```

\newpage

## Naive Bayes Network

```{r}
library(mlbench)
naive <- naiveBayes(diag ~ ., data = parkinson_train)
y_naive<-predict(naive,newdata = parkinson_test)
A3<-mean(y_naive==parkinson_test$diag)
R3<-mean(y_naive[parkinson_test$diag=='TRUE']==parkinson_test[parkinson_test$diag=='TRUE',]$diag)
```

For example, the marginal distribution of `gender` is shown as following.

```{r results='asis'}
kable(round(naive$tables$gender[,c(1,2)],2))
```



## Random Forest

```{r}
library(randomForest)
fit_rf<-randomForest(diag~.,data = parkinson_train)
rfpred<-predict(fit_rf,newdata = parkinson_test)
A4<-mean(rfpred==parkinson_test$diag)
R4<-mean(rfpred[parkinson_test$diag=='TRUE']==parkinson_test[parkinson_test$diag=='TRUE',]$diag)
```

Even a small tree can have a high accuracy.

```{r}
library(party)
x <- ctree(diag~brain+smoke, data=parkinson_train)
xpred<-predict(x,newdata = parkinson_test)
round(mean(xpred==parkinson_test$diag),2)
plot(x, type="simple")
```



## XGBoost

```{r}
library(xgboost)
data_train<-model.matrix(~.+0,data = parkinson_train[,1:8])
data_test<-model.matrix(~.+0,data = parkinson_test[,1:8])
dtrain <- xgb.DMatrix(data = data_train,label = ifelse(parkinson_train$diag=='TRUE',1,0))
dtest <- xgb.DMatrix(data = data_test,label = ifelse(parkinson_test$diag=='TRUE',1,0))
params <- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=0, 
               max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5, showsd = T, 
                 stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)
fit_xgb<-xgb.train(data = dtrain, max_depth = 6, eta = 0.3, nthread = 2, nrounds = 11,
                   objective = "binary:logistic")
xgpred<-predict(fit_xgb,newdata = dtest)
A5<-mean(ifelse(xgpred<=0.5,0,1)==ifelse(parkinson_test$diag=='TRUE',1,0))
R5<-mean(ifelse(xgpred<=0.5,0,1)[parkinson_test$diag=='TRUE']==
           ifelse(parkinson_test$diag=='TRUE',1,0)[parkinson_test$diag=='TRUE'])
```

Importance of each predictor is shown as following.

```{r warning=FALSE}
mat <- xgb.importance(feature_names = colnames(data_train),model=fit_xgb)
xgb.plot.importance (importance_matrix = mat)
```

\newpage

## Summary

Comparison of the 5 methods is shown as following.

```{r results='asis'}
kable(data.frame(Method=c('logistic','SVM','Naive Bayes','Random Forest','XGBoost'),
                 Accuracy=round(c(A1,A2,A3,A4,A5),2),
                 Recall=round(c(R1,R2,R3,R4,R5),2)))
```

\newpage

# Combining with Voice Features 

```{r}
setwd("~/Parkinson_Classification_Based_on_Demographic_Information_and_Voice_Features")
train <- read.csv("./src/R/train.csv", header=FALSE)
test <- read.csv("./src/R/test.csv", header=FALSE)
mean(train[,1]==1)
```

## Baseline - Logistic Regression Based on Demographic Information Or Voice Features

Combined dataset is decoded with dummy variables. There are 50107 records and 102 features (62 voice features).

```{r}
model_b1<-glm(V1~.,data=train[,1:41],family=binomial(link='logit'))
y_b1<-predict.glm(model_b1,newdata = test,type='response')
y_b1=ifelse(y_b1>=0.5,1,0)
(A_b1<-mean(y_b1==test$V1))
(R_b1<-mean(y_b1[test$V1==0]==test[test$V1==0,]$V1))

model_b2<-glm(V1~.,data=train[,c(1,42:103)],family=binomial(link='logit'))
y_b2<-predict.glm(model_b2,newdata = test,type='response')
y_b2=ifelse(y_b2>=0.5,1,0)
(A_b2<-mean(y_b2==test$V1))
(R_b2<-mean(y_b2[test$V1==0]==test[test$V1==0,]$V1))
```

## Logistic Regression with Regularization (Lasso)

Penalty parameter $\lambda$ is chosed based on cross validation.

```{r}
library(glmnet)
lasso_cv<-cv.glmnet(x=as.matrix(train[,-1]),y=train[,1],alpha = 1,family="binomial")
model2<-glmnet(x=as.matrix(train[,-1]),y=train[,1],alpha = 1,family="binomial",
                    lambda = lasso_cv$lambda.min)
(A<-mean(predict(model2,newx=as.matrix(test[,-1]),type="class")==test[,1]))
(R<-mean(predict(model2,newx=as.matrix(test[,-1]),type="class")[test[,1]==0]==test[test[,1]==0,1]))
```

Variable selection can be shown as following.

```{r}
plot(lasso_cv)
```

Regularization paths can be shown as following.

```{r}
p_lasso<-glmnet(x=as.matrix(train[,-1]),y=train[,1],alpha = 1,family="binomial")
plot(p_lasso)
```

## Final Model - Hierarchical Regularized Logistic Regression

Regularized logistic regression fails to make a big improvement. One issue is that messing up demographic information and voice features may not be wise.

Finally, I am thinking in a hierarchical framework. I am going to group patients by their age, which is shown to be a strong demographic predictor. And I build 3 regularized logistic regression models with other covariates (both demographics and voice features) accordingly.

```{r}
group.train<-c()
group.test<-c()

for (i in 1:nrow(train)){
  if (train[i,]$V2<=55) group.train<-c(group.train,1)
  if (train[i,]$V2>55&train[i,]$V2<=68) group.train<-c(group.train,2)
  if (train[i,]$V2>68) group.train<-c(group.train,3)
}

for (i in 1:nrow(test)){
  if (test[i,]$V2<=55) group.test<-c(group.test,1)
  if (test[i,]$V2>55&test[i,]$V2<=68) group.test<-c(group.test,2)
  if (test[i,]$V2>68) group.test<-c(group.test,3)
}

train.bayes<-data.frame(cbind(level=train$V1,group=group.train,apply(train[,3:103], MARGIN=2, FUN = function(X) (X - mean(X)) / sd(X))))
test.bayes<-data.frame(cbind(level=test$V1,group=group.test,apply(test[,3:103], MARGIN=2, FUN = function(X) (X - mean(X)) / sd(X))))

train.bayes1<-train.bayes[train.bayes$group==1,]
train.bayes2<-train.bayes[train.bayes$group==2,]
train.bayes3<-train.bayes[train.bayes$group==3,]
```

```{r}
#lasso_cv1<-cv.glmnet(x=as.matrix(train.bayes1[,-(1:2)]),y=train.bayes1[,1],alpha = 1,family="binomial")
modelFit1<-glmnet(x=as.matrix(train.bayes1[,-(1:2)]),y=train.bayes1[,1],alpha = 1,family="binomial",
                    lambda = 0.005)
#lasso_cv2<-cv.glmnet(x=as.matrix(train.bayes2[,-(1:2)]),y=train.bayes2[,1],alpha = 1,family="binomial")
modelFit2<-glmnet(x=as.matrix(train.bayes2[,-(1:2)]),y=train.bayes2[,1],alpha = 1,family="binomial",
                    lambda = 0.0002)
#lasso_cv3<-cv.glmnet(x=as.matrix(train.bayes3[,-(1:2)]),y=train.bayes3[,1],alpha = 1,family="binomial")
modelFit3<-glmnet(x=as.matrix(train.bayes3[,-(1:2)]),y=train.bayes3[,1],alpha = 1,family="binomial",
                    lambda = 0.0005)
```

```{r}
predFit<-c()
predFit1<-c()

for (i in 1:nrow(test.bayes)){
  if (test.bayes[i,]$group==1){
    predFit<-c(predFit,predict(modelFit1,newx=as.matrix(test.bayes[i,-(1:2)]),type="class"))
    predFit1<-c(predFit1,predict(modelFit1,newx=as.matrix(test.bayes[i,-(1:2)]),type="response"))
  } 
  if (test.bayes[i,]$group==2){
    predFit<-c(predFit,predict(modelFit2,newx=as.matrix(test.bayes[i,-(1:2)]),type="class"))
    predFit1<-c(predFit1,predict(modelFit2,newx=as.matrix(test.bayes[i,-(1:2)]),type="response"))
  }
  if (test.bayes[i,]$group==3){
    predFit<-c(predFit,predict(modelFit3,newx=as.matrix(test.bayes[i,-(1:2)]),type="class"))
    predFit1<-c(predFit1,predict(modelFit3,newx=as.matrix(test.bayes[i,-(1:2)]),type="response"))
  }
}

(AA<-mean(predFit==test.bayes$level))
(RR<-mean(predFit[test[,1]==0]==test[test[,1]==0,1]))
```

ROC curve is shown as following.

```{r}
library(ROCR)
pred1<-predict(modelFit1,newx=as.matrix(test.bayes[test.bayes$group==1,-(1:2)]),type='response')
pred2<-predict(modelFit2,newx=as.matrix(test.bayes[test.bayes$group==2,-(1:2)]),type='response')
pred3<-predict(modelFit3,newx=as.matrix(test.bayes[test.bayes$group==3,-(1:2)]),type='response')
pred1 <- prediction(pred1, test.bayes[test.bayes$group==1,]$level)
pred2 <- prediction(pred2, test.bayes[test.bayes$group==2,]$level)
pred3 <- prediction(pred3, test.bayes[test.bayes$group==3,]$level)
predFit2 <- prediction(predFit1, test.bayes$level)
perf1 <- performance(pred1,"tpr","fpr")
perf2 <- performance(pred2,"tpr","fpr")
perf3 <- performance(pred3,"tpr","fpr")
perf4 <- performance(predFit2,"tpr","fpr")
plot(perf1,colorize=FALSE, col=1,main='ROC for hierarchical model')
par(new=TRUE)
plot(perf2,colorize=FALSE, col=2,main='ROC for hierarchical model')
par(new=TRUE)
plot(perf3,colorize=FALSE, col=3,main='ROC for hierarchical model')
par(new=TRUE)
plot(perf4,colorize=FALSE, col=4,main='ROC for hierarchical model')
lines(c(0,1),c(0,1),col = "gray", lty = 4 )
legend('bottomright', legend=c('Age<=55',"Age56~68", "Age>68","Hierarchical"), col=c(1:4), lty=1, cex=0.8)
```

## Summary


```{r results='asis'}
kable(data.frame(Method=c('baseline1 - Demographics','baseline2 - Voice','Regularized Logistic','Hierarchical Regularized Logistic'),
                 Accuracy=round(c(A_b1,A_b2,A,AA),3),
                 Recall=round(c(R_b1,R_b2,R,RR),3)))
```

\newpage

## Original Computational Environment {-}

```{r}
sessionInfo()
```
